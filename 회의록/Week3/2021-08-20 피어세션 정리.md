# 2021-08-20 피어세션

- 학습 관련 QnA

## 학습 관련 QnA

#### Q_김재영 : transformer에 들어가는 임베딩 vector를 고르는 방법이 있는가?

A_김범찬 : 다양한 방법이 존재함. activation function으로 sigmoid, ReLU를 사용할 수 있는 것처럼 각각의 vector를 만드는 방법에도 장단점이 있음. 적절한 방법으로 임베딩 vector를 골라주면 될 듯

#### Q_김범찬 : [질문 - 과제 1의 module_hook에서 print(grad_output)을 하면 1이 나오는 이유?](https://github.com/jinhan814/BoostCamp-AITech2-Level1-20/blob/main/%ED%9A%8C%EC%9D%98%EB%A1%9D/Week3/2021-08-19%20%ED%94%BC%EC%96%B4%EC%84%B8%EC%85%98%20%EC%A0%95%EB%A6%AC.md#q_%EA%B9%80%EB%B2%94%EC%B0%AC--%EA%B3%BC%EC%A0%9C-1%EC%9D%98-module_hook%EC%97%90%EC%84%9C-printgrad_output%EC%9D%84-%ED%95%98%EB%A9%B4-1%EC%9D%B4-%EB%82%98%EC%98%A4%EB%8A%94-%EC%9D%B4%EC%9C%A0)

A_최태종 : output에 대해 backward를 진행하는데 backward_hook을 전체 모델에 적용 해줬기때문에 1로 나오는 것이다. 중간레이어에 대해 backward_hook을 적용 했다면 1이 아닌 다른값이 나올것이다.

## dropout의 원리 (박진한)

1. 여러 개의 모델을 학습시킨 뒤 평균을 내는 것과 비슷한 효과를 낸다.

2. 각 뉴런간의 의존도를 낮춰서 세부적인 특징을 더 잘 학습할 수 있도록 한다.

- 참고 : [http://neuralnetworksanddeeplearning.com/chap3.html#overfitting_and_regularization](http://neuralnetworksanddeeplearning.com/chap3.html#overfitting_and_regularization)

## TODO LIST

* 김민성_T2024
  * .

* 김범찬_T2031
  * 주간학습정리 제출
  * 아그파서 비비기

* 김재영_T2049
  * .

* 김정현_T2051
  * .

* 박진한_T2097
  * .

* 최태종_T2232
  * .

* 한재현_T2236
  * .
