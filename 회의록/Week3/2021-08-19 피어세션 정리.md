# 2021-08-19 피어세션

- 학습 관련 QnA

- 알고리즘 문제 풀이 발표 (박진한)

## 학습 관련 QnA

#### Q_김범찬 : 과제 1의 module_hook에서 print(grad_output)을 하면 1이 나오는 이유?

A 최태종. 자기 자신으로 미분했기 때문으로 추정[https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html](링크서 Autograd 부분 참조)

#### Q_김범찬 : 과제 1의 apply에서 weight_initialization 함수에서 module_name이 parameter로 있는 이유?

A 김재영. <code>try, except</code>를 이용하여 weight를 가지고 있지 않은 경우에만 weight를 update를 해줄 수도 있겠지만 (module_name 사용 X), <code>if "Function" in module_name:</code> 와 같이 Function module의 weight를 update해줄 수도 있기 때문.

#### Q_김범찬 : xavier, he initialization의 수식의 직관적인 이해? 두 방법이 각각 sigmoid, relu 계열의 함수에 적합한 이유는?

#### Q_김범찬 : dropout을 쓰면 더 잘되는 경향이 있는 까닭은?

A 박진한. 뉴런 간의 co-adaptation이 감소하고, 여러 개의 Network를 사용하는 효과를 기대할 수 있기 때문

## 알고리즘 문제 풀이 발표 (박진한)

- DP란? : <sup>1)</sup>큰 문제를 부분 문제들의 답으로 풀 수 있으며 <sup>2)</sup>부분 문제가 여러 번 사용되는 경우 사용할 수 있는 방법.

- memoization : 메모이제이션, 부분 문제의 답을 저장함.

- state : 현재 풀어야 하는 문제의 상태를 어떤 값(거의 대부분 정수)을 이용해 표현할 수 있어야 함. (memoization을 위함)

- 점화식 : 문제의 상태를 표현하는데 성공했다면 큰 문제를 부분 문제들로 풀기 위해서 state간의 상태 전이를 식으로 표현해야 함. (이를 점화식이라 부름)

- 기저사례 : 점화식에서 base가 되는 state들의 값을 미리 지정해둬야 함.

- -> 여기까지 state, 점화식을 잘 정의했다면 이제 구현을 하면 됨. (Bottom Up / Top Down)

## TODO LIST

* 김민성_T2024
  * .

* 김범찬_T2031
  * .

* 김재영_T2049
  * .

* 김정현_T2051
  * .

* 박진한_T2097
  * Xavier, He initialization 원리 정리하기
  * Data Augmentation - MNIST 손글씨 이미지 데이터 평행이동, 회전이동, scaling 구현하기
  * numpy로 CNN 모델 구현하기
  * 과제 1 마무리
  * [http://neuralnetworksanddeeplearning.com/chap3.html](http://neuralnetworksanddeeplearning.com/chap3.html) 읽어보기
  * MLP 모델에 dropout 구현해보기
  * 회의록 작성 마무리하기

* 최태종_T2232
  * .

* 한재현_T2236
  * .
