# 2021-08-18 피어세션

- 학습 관련 QnA

- Learning rate Scheduler 발표 (김범찬)

- MLP 구현에서 gradient exploding의 원인과 해결 방안 (박진한)

## 학습 관련 QnA

#### Q_김범찬 : class의 상속에서 super의 사용 이유? nn.Module에서 super.init()이란?

A. 파이썬의 특징. 파이썬에서 class 내부 메소드는 self를 이용하는 것처럼 부모 클래스에 접근할 때에 super를 이용하도록 구현됨.

A. nn.Module - super.init() : [참고 링크](https://algopoolja.tistory.com/39)

#### Q_김재영 : hook이란?

A. 최태종 : 모델을 실행시키면서 자기가 만든 custom 함수를 중간에 실행시킬 수 있도록 하는게 hook.

## Learning rate Scheduler 발표 (김범찬)

- Scheduler_Practice.ipynb 참고

## MLP 구현에서 gradient exploding의 원인과 해결 방안 (박진한)

- 원인 : Wx + b 결과가 점점 커지면서 x가 발산함. sigmoid 함수를 이용할 때엔 Sigmoid(Wx + b)가 0 ~ 1 사이 값이라 x의 절댓값이 커지지 않았지만, ReLU를 사용하면 Wx + b가 점점 커진다는 문제점이 발생함.

- 해결방안 : He initialization, L2 Regularization, lr : 0.1 -> 0.03을 이용해서 Wx + b의 절댓값이 엄청 커지는 것을 방지. 특히 He initialization을 사용해서 정상적으로 학습이 되는 모델에서도 He initialization 부분을 제거하면 W, b값이 발산하는 것으로 보아서 ReLU 함수를 이용할 땐 initialization이 특히 중요하다고 생각할 수 있음. grad clip, cost function을 cross entropy로 변경, L1 Regularization, ReLU 함수를 조금 수정해 max(0, 0.1 * x) 등을 이용하기 등 다양한 추가 해결 방안이 회의중에 제안되었음. 나중에 해당 방법들도 시도해볼 예정.

## TODO LIST

* 김민성_T2024
  * .

* 김범찬_T2031
  * .

* 김재영_T2049
  * .

* 김정현_T2051
  * .

* 박진한_T2097
  * .

* 최태종_T2232
  * .

* 한재현_T2236
  * .
 
