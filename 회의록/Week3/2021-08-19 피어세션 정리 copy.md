# 2021-08-19 피어세션

- 학습 관련 QnA

- 알고리즘 문제 풀이 발표 (박진한)

## 학습 관련 QnA

#### Q_김범찬 : 과제 1의 module_hook에서 print(grad_output)을 하면 1이 나오는 이유?

#### Q_김범찬 : 과제 1의 apply에서 weight_initialization 함수에서 module이 parameter로 있는 이유?

#### Q_김범찬 : xavier, he initialization의 수식의 직관적인 이해? 두 방법이 각각 sigmoid, relu 계열의 함수에 적합한 이유는?

#### Q_김범찬 : dropout의 원리?

## 알고리즘 문제 풀이 발표 (박진한)

- DP란? : <sup>1)</sup>큰 문제를 부분 문제들의 답으로 풀 수 있으며 <sup>2)</sup>부분 문제가 여러 번 사용되는 경우 사용할 수 있는 방법.

- memoization : 메모이제이션, 부분 문제의 답을 저장함.

- state : 현재 풀어야 하는 문제의 상태를 어떤 값(거의 대부분 정수)을 이용해 표현할 수 있어야 함. (memoization을 위함)

- 점화식 : 문제의 상태를 표현하는데 성공했다면 큰 문제를 부분 문제들로 풀기 위해서 state간의 상태 전이를 식으로 표현해야 함. (이를 점화식이라 부름)

- 기저사례 : 점화식에서 base가 되는 state들의 값을 미리 지정해둬야 함.

- -> 여기까지 state, 점화식을 잘 정의했다면 이제 구현을 하면 됨. (Bottom Up / Top Down)

## TODO LIST

* 김민성_T2024
  * .

* 김범찬_T2031
  * .

* 김재영_T2049
  * .

* 김정현_T2051
  * .

* 박진한_T2097
  * Xavier, He initialization 원리 정리하기
  * Data Augmentation - MNIST 손글씨 이미지 데이터 평행이동, 회전이동, scaling 구현하기
  * numpy로 CNN 모델 구현하기
  * 과제 1 마무리
  * [http://neuralnetworksanddeeplearning.com/chap3.html](http://neuralnetworksanddeeplearning.com/chap3.html) 읽어보기
  * MLP 모델에 dropout 구현해보기
  * 회의록 작성 마무리하기

* 최태종_T2232
  * .

* 한재현_T2236
  * .
