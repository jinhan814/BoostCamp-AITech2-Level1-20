# 2021-08-17 피어세션

- 학습 관련 QnA

- Neural Network 구현 발표 (박진한)

## 학습 관련 QnA

#### Q_김범찬 : SGD 구현에서 Label이 random shuffle되지 않고 sorted되어있더라도, SGD의 기본원리에 따라 학습이 잘 될까?

A.  

김범찬 : 표본집단의 평균의 평균은 모집단의 평균과 같기 때문에 상관없을 거 같다.

김재영 : label이 0인 objective function에 맞춘 뒤 1인 objective function, ..., 이러면 뭔가 optimal한 값이 안 얻어질 거 같다. 각각의 convex function에서 local minima로 이동하며 학습하면 성능이 나빠질 것으로 생각한다.

박진한 : label이 같은 데이터만으로 학습을 쭉 시켜두면 sigmoid 함수의 특징 상 포화가 되면서 gradient가 0이 되기 때문에 잘못된 값에 포화가 되어서 성능이 떨어질 것 같다.

#### Q_최태종 : NN 구현에서 zeros_likes 쓰면 더 낫지 않을까?

A. 박진한 : 더 깔끔한 구현이 가능할 거 같다.

#### Q_박진한 : NN 구현에서 ReLU를 적용하면 음의 무한대로 발산하던데 이유를 모르겠다.

A. 김범찬: Activation Function을 바꾸면 learning rate도 바꿔줘야 한다. ReLU에서 편미분값이 양수일 때 lr이 너무 크다면 w, b를 너무 많이 줄이게 되면서 음의 무한대로 발산할 수 있다.

#### Q_김재영 : 패키지화, config 파일 작성하는 팁? (강의 3)

A. 김정현 : dataLoader부터 시작해서 Model.py, train.py, test.py로 시작하면 간단하다.

## Neural Network 구현 발표 (박진한)

- [https://github.com/jinhan814/NN_implementation](https://github.com/jinhan814/NN_implementation)

## TODO LIST

* 김민성_T2024
  * 오늘 필수과제 끝내기
  * Transformer 논문 나머지 정리
  * 시간 남으면 시각화 강의 수강

* 김범찬_T2031
  * 회의록에 본인 비중 추가
  * 1~3강 수강 완료 및 정리
  * Data Shuffle 관련해서 데이터 처리 비비기

* 김재영_T2049
  * .

* 김정현_T2051
  * 1~3강 정리
  * ProGAN PyTorch 구현해보기
  * 알고리즘 정리 (피보나치)

* 박진한_T2097
  * 회의록 작성하기
  * MLP 구현 코드에 ReLU 적용시켜보기
  * 1~3강 정리

* 최태종_T2232
  * Attention is all you Need 읽기
  * 알고리즘 문제 풀기

* 한재현_T2236
  * .
